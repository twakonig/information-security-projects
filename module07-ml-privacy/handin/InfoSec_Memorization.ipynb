{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "-HsHGPWZBFtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ],
      "metadata": {
        "id": "QHH8AofBE9Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae823f3-ff6b-4b89-825b-b74811341342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Download the lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ],
      "metadata": {
        "id": "qkfrTYZ7BHBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23e95ff-6717-4008-b22c-58922ba328d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infoseclab'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 321 (delta 13), reused 31 (delta 10), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (321/321), 64.87 MiB | 21.03 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "/content/infoseclab\n",
            "From https://github.com/ethz-privsec/infoseclab\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3UFYO94QBIKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import infoseclab\n",
        "from infoseclab import extraction, Vocab, PREFIX\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# we won't need gradients here so let's disable them to make things faster\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# utilities for loading & saving results\n",
        "def read_results():\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"r\") as f:\n",
        "    res = json.load(f)\n",
        "  return res\n",
        "\n",
        "\n",
        "def write_results(res):\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"w\") as f:\n",
        "    res = json.dump(res, f)\n",
        "\n",
        "\n",
        "def print_results(res):\n",
        "  for key, value in res.items():\n",
        "    print(f\"{key.replace('_', ' ')}: {repr(value)}\")"
      ],
      "metadata": {
        "id": "qN2qQU8dBMG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create file to save results"
      ],
      "metadata": {
        "id": "0xYlh_fn7ETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  res = read_results()\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "except FileNotFoundError:\n",
        "  res = {\n",
        "      \"main_character\": None,\n",
        "      \"greedy_guess\": None,\n",
        "      \"greedy_numeric_guess\": None,\n",
        "      \"exact_guess\": None\n",
        "  }\n",
        "  write_results(res)\n",
        "\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "AmNr00RV7D4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa20cc7d-7b20-4720-b0df-e86afe2ca17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Freeform generation\n",
        "\n",
        "We will be working with a simple *character-level* language model.\n",
        "\n",
        "This is a model that takes as input a sentence (e.g., \"my name is \") and outputs a distribution over the next character in the sentence. We can then generate a character (e.g., \"F\") by sampling from this distribution. By applying the model recursively to its own output we can generate text character by character: \"my name is Florian\".\n",
        "\n",
        "Technically, the langauge model doesn't operate on `characters` but on `tokens` (numbers). The characters in the model's \"vocabulary\" are sorted, and can thus be referenced by an integer. The i-th value in the langauge model's output corresponds to the probability assigned to the i-th character in the vocabulary.\n",
        "\n",
        "You can find the full vocabulary (i.e., all characters that the language model can produce) in `infoseclab.extraction.Vocab`.\n",
        "This class has two utility dictionaries, `char_to_ix` and `ix_to_char` for converting from a character to its index (its token) and vice-versa:\n",
        "\n",
        "```\n",
        "Vocab.char_to_ix['a'] -> 54\n",
        "Vocab.ix_to_char[54] -> 'a'\n",
        "```"
      ],
      "metadata": {
        "id": "Y1PXWtQ-BnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a simple character-level language model\n",
        "lm = extraction.load_lm(\"infoseclab/data/secret_model.pth\", device=device)"
      ],
      "metadata": {
        "id": "3z_zuKMQ37V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to generate text from the language model\n",
        "extraction.generate(lm, \"The Adventures of \", length=300)"
      ],
      "metadata": {
        "id": "qT7c-NI57MSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "974805aa-592e-4cf5-fee0-2db7279adc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Adventures of Stake\\n Tremor the lady's house? Mycron, and that him to him with a sticken of\\n valuard acquainting also on Miss Cushing is Viville Cungal of\\n Oxshott made Sarah English tale. Was the detection when it is near with client no\\n doings of our aloud feet. Both misses him. I was essential\\n matry of object\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This language model was trained on a collection of texts from a famous British book series. \n",
        "Your first goal is to figure out which books.**\n",
        "\n",
        "**Your guess should be in the form `\"Firstname Lastname\"` of the book series' main character.\n",
        "For example, if you guessed that the book series is Harry Potter, then your guess would be `\"Harry Potter\"`.**\n",
        "\n",
        "Note: the code immediately below doesn't check for correctness! It just checks that you've made a guess."
      ],
      "metadata": {
        "id": "zKL8VWwn4gVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "guess = \"Sherlock Holmes\"\n",
        "res = read_results()\n",
        "res['main_character'] = guess\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "cXRYQ3B6Bpti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9298c80-281a-4269-ed9c-8c9e46f19ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Secret extraction\n",
        "\n",
        "Unfortunately, the training data from this language model also contained the sentence `\"Florian's password is XXXXX\"`. (the real password is blanked out, your goal is to recover it!)\n",
        "\n",
        "The model might have *memorized* the correct password, and your goal will be to recover it.\n",
        "\n",
        "For this, you know the *prefix*: `\"Florian's password is \"`\n",
        "(you can find this stored under `infoseclab.extraction.PREFIX`).\n",
        "\n",
        "You also know that Florian's password is exactly 5 characters long (so that it it easier to memorize, *obviously*)."
      ],
      "metadata": {
        "id": "K3c1YfON5bx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1&nbsp; Greedy secret extraction\n",
        "\n",
        "You will first attempt to extract the secret password *greedily*, simply by sampling the **5 most likely characters**, one-by-one, from the language model, starting from the known `PREFIX`.\n",
        "\n",
        "You can use the `extraction.generate` method as inspiration for this.\n",
        "\n",
        "*Note that `extraction.generate` does <b>not</b> sample greedily from the model. Rather, it samples a character at random according to the probability distribution predicted by the model.*"
      ],
      "metadata": {
        "id": "XLR_DCUHDt5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy(lm, prompt, length=5):\n",
        "\n",
        "    # adapted code from infoseclab/extraction.py, generate()\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "\n",
        "        # sample a character according to the predicted distribution\n",
        "        # print(\"Max confidence: \", torch.max(probas))\n",
        "        index = torch.argmax(probas)\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "\n",
        "guess_greedy = generate_greedy(lm, PREFIX, length=5)\n",
        "print(\"greedy:\", PREFIX + repr(guess_greedy))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_guess'] = guess_greedy\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "2tq-2nO8D0Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad6c23a-6010-4f64-a817-06cbfa28eedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: Florian's password is '3\\n an'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2&nbsp;Greedy numeric secret extraction\n",
        "\n",
        "Your greedy extraction likely generated some giberish! (but hey, a password might genuinely look like that).\n",
        "\n",
        "You are now given some extra information: **Florian's password only contains numbers!** (he's not very good at security).\n",
        "\n",
        "Modify your greedy sampling mechanism to repeatedly sample the 5 most likely *numbers*, one-by-one, starting from the known `PREFIX`."
      ],
      "metadata": {
        "id": "wCtm2C2L5jep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "\n",
        "     # adapted code from infoseclab/extraction.py, generate()\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "        num_probas = probas[12:22]\n",
        "\n",
        "        # take character with max. 'confidence'\n",
        "        index = torch.argmax(num_probas) + 12\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "6Pbzx4dSa1sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780cebfc-122c-4659-b997-e3e65b0dfcef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy (numeric): Florian's password is '39731'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3&nbsp;Exact numeric secret extraction\n",
        "\n",
        "Spoiler alert: the secret you found using greedy sampling is *not* Florian's password.\n",
        "\n",
        "As it turns out, sampling greedily from the model is not guaranteed to find the *sequence* of characters that is most likely according to the model's probability distribution.\n",
        "\n",
        "To illustrate, below you can compare the loss from your greedy guess, and a different (also incorrect) guess.</br>\n",
        "The guess `\"36175\"` has lower loss!"
      ],
      "metadata": {
        "id": "16tSQO1RHBxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(guess_greedy_numeric, extraction.get_loss(lm, PREFIX + guess_greedy_numeric))\n",
        "print(\"36175\", extraction.get_loss(lm, PREFIX + \"36175\"))"
      ],
      "metadata": {
        "id": "J5xuvMF7HFg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995c9e12-52b4-4497-ec66-f9b024c6a9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39731 tensor(0.9791, device='cuda:0')\n",
            "36175 tensor(0.8980, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the final part, find the 5-digit secret that actually *minimizes* the model's loss, when prompted with the `PREFIX`."
      ],
      "metadata": {
        "id": "IkmUuKQWbaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_exact(lm, prompt, length=5):\n",
        "\n",
        "    #---------------------------TRY EXHAUSTIVE PW CREATION HERE--------------------\n",
        "    best_guess = \"\"\n",
        "    min_loss = 100\n",
        "\n",
        "    for a in range(10):\n",
        "      print(\"a = \", a)\n",
        "      for b in range(10):\n",
        "        for c in range(10):\n",
        "          for d in range(10):\n",
        "            for e in range(10):\n",
        "              pw = str(a) + str(b) + str(c) + str(d) + str(e)\n",
        "              curr_loss = extraction.get_loss(lm, PREFIX + pw)\n",
        "              if curr_loss < min_loss:\n",
        "                min_loss = curr_loss\n",
        "                best_guess = pw\n",
        "\n",
        "    print(\"min_loss = \", min_loss)\n",
        "\n",
        "    return best_guess\n",
        "\n",
        "\n",
        "\n",
        "guess_exact = generate_exact(lm, PREFIX, length=5)\n",
        "print(\"\\nexact:\", PREFIX + repr(guess_exact))\n",
        "\n",
        "res = read_results()\n",
        "res['exact_guess'] = guess_exact\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "CjLjFgyTIzgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53dd88f6-4f61-4b7a-933a-4f43f9f33fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a =  0\n",
            "a =  1\n",
            "a =  2\n",
            "a =  3\n",
            "a =  4\n",
            "a =  5\n",
            "a =  6\n",
            "a =  7\n",
            "a =  8\n",
            "a =  9\n",
            "min_loss =  tensor(0.5321, device='cuda:0')\n",
            "\n",
            "exact: Florian's password is '35192'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**) \n"
      ],
      "metadata": {
        "id": "fNMIfOoL_dOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ],
      "metadata": {
        "id": "S0N1Uv1Y_cLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b55dfef-8bfb-4dc5-a2f5-adf0f9bb82f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: x_adv_targeted.npy (deflated 10%)\n",
            "  adding: x_adv_detect.npy (deflated 10%)\n",
            "  adding: extraction.json (deflated 25%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    res = json.load(zip.open(\"extraction.json\"))\n",
        "    print_results(res)"
      ],
      "metadata": {
        "id": "VSPUajuP_zcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958e25b7-0bb8-4533-cb81-de8725c81274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    }
  ]
}